{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Text Preprocessing Pipeline for Bahasa\n",
    "The process includes:\n",
    "\n",
    "1. Casefolding (lowercase, remove numbers, remove punctuation, remove non-ascii character);\n",
    "2. Strip HTML, remove url, remove email;\n",
    "3. Normalize slang world;\n",
    "4. Stemming;\n",
    "5. Tokenize;\n",
    "6. Filtering / stopword removal.\n",
    "\n",
    "2020 &copy; Kuncahyo Setyo Nugroho <br/>\n",
    "Faculty of Computer Science, Brawijaya University\n",
    "\n",
    "Present in PyCon ID Online 2020, 13 November 2020"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "class TextPreprocessing():\n",
    "    def __init__(self, text = 'test'):\n",
    "        self.text = text\n",
    "    \n",
    "    def lowercase(self):\n",
    "        \"\"\"Convert to lowercase\"\"\"\n",
    "        self.text = self.text.lower()\n",
    "        self.text = self.text.strip()\n",
    "        return self\n",
    "\n",
    "    def strip_html(self):\n",
    "        \"\"\"HTML tag removal\"\"\"\n",
    "        soup = BeautifulSoup(self.text, 'lxml')\n",
    "        self.text = soup.get_text()\n",
    "        return self\n",
    "\n",
    "    def remove_url(self):\n",
    "        \"\"\"Remove URL (http/https/www) or custom URL\"\"\"\n",
    "        self.text = re.sub(r'https?://\\S+|www\\.\\S+', '', self.text)\n",
    "        self.text = re.sub(r'pic.twitter.com\\S+', '', self.text) # custom for twitter\n",
    "        return self\n",
    "    \n",
    "    def remove_email(self):\n",
    "        \"\"\"Remove email\"\"\"\n",
    "        self.text = re.sub('\\S*@\\S*\\s?', '', self.text)\n",
    "        return self\n",
    "    \n",
    "    def remove_between_square_brackets(self):\n",
    "        \"\"\"Remove string beetwen square brackets []\"\"\"\n",
    "        self.text = re.sub('\\[[^]]*\\]', '', self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_numbers(self):\n",
    "        \"\"\"Remove numbers\"\"\"\n",
    "        self.text = re.sub('[-+]?[0-9]+', '', self.text)\n",
    "        return self\n",
    "    \n",
    "    def remove_emoji(self):\n",
    "        \"\"\"Remove emoji, e.g ðŸ˜œðŸ˜€ \"\"\"\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        self.text = emoji_pattern.sub(r'', self.text)\n",
    "        return self\n",
    "    \n",
    "    def remove_emoticon(self):\n",
    "        \"\"\"Remove emoticon, e.g :-)\"\"\"\n",
    "        emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "        self.text = emoticon_pattern.sub(r'', self.text)\n",
    "        return self\n",
    "    \n",
    "    def convert_emoji(self):\n",
    "        \"\"\"Convert emoji to word\"\"\"\n",
    "        for emoji in UNICODE_EMO:\n",
    "            self.text = self.text.replace(emoji, '_'.join(UNICODE_EMO[emoji].replace(',','').replace(':','').split()))\n",
    "        return self\n",
    "    \n",
    "    def convert_emoticon(self):\n",
    "        \"\"\"Convert emoticon to word\"\"\"\n",
    "        for emoticon in EMOTICONS:\n",
    "            self.text = re.sub(u'('+emoticon+')', '_'.join(EMOTICONS[emoticon].replace(',','').split()), self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        \"\"\"Remove punctuation\"\"\"\n",
    "        self.text = re.sub(r'[^\\w\\s]', '', self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_non_ascii(self):\n",
    "        \"\"\"Remove non-ascii character\"\"\"\n",
    "        self.text = unicodedata.normalize('NFKD', self.text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return self\n",
    "\n",
    "    def normalize_word(self):\n",
    "        \"\"\"Normalize slang world\"\"\"\n",
    "        normal_word_path = pd.read_csv('key_norm.csv')\n",
    "        \n",
    "        self.text = ' '.join([normal_word_path[normal_word_path['singkat'] == word]['hasil'].values[0] \n",
    "        if (normal_word_path['singkat'] == word).any() else word for word in self.text.split()])\n",
    "        return self\n",
    "\n",
    "    def stemming(self):\n",
    "        \"\"\"Stemming for Bahasa with Sastrawi\"\"\"\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "\n",
    "        self.text = stemmer.stem(self.text)\n",
    "        return self\n",
    "\n",
    "    def tokenize(self):\n",
    "        \"\"\"Tokenize words\"\"\"\n",
    "        self.words = nltk.word_tokenize(self.text)\n",
    "        return self\n",
    "\n",
    "    def stopwords_removal(self):\n",
    "        \"\"\"Stopword removal\"\"\"\n",
    "        stopword = stopwords.words('indonesian')\n",
    "        more_stopword = ['daring', 'online', 'pd'] # add more stopword to default corpus\n",
    "        stop_factory = stopword + more_stopword\n",
    "        \n",
    "        clean_words = []\n",
    "        for word in self.words:\n",
    "            if word not in stop_factory:\n",
    "                clean_words.append(word)\n",
    "        self.words = clean_words\n",
    "        return self\n",
    "    \n",
    "    def join_words(self):\n",
    "        \"\"\"Jonin all words\"\"\"\n",
    "        self.words = ' '.join(self.words)\n",
    "        return self\n",
    "    \n",
    "    def do_all(self, text):\n",
    "        \"\"\"Do all text preprocessing process\"\"\" # or custom process\n",
    "        self.text = text\n",
    "        self = self.lowercase()\n",
    "        self = self.strip_html()\n",
    "        self = self.remove_url()\n",
    "        self = self.remove_email()\n",
    "        self = self.remove_between_square_brackets()\n",
    "        self = self.remove_numbers()\n",
    "        self = self.remove_emoticon()\n",
    "        self = self.remove_emoji()\n",
    "        self = self.convert_emoticon()\n",
    "        self = self.convert_emoji()\n",
    "        self = self.remove_punctuation()\n",
    "        self = self.remove_non_ascii()\n",
    "        self = self.normalize_word()\n",
    "        self = self.stemming()\n",
    "        self = self.tokenize()\n",
    "        self = self.stopwords_removal()\n",
    "        self = self.join_words()\n",
    "        return self.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "drama kemarin sore puasa pas mandi kaget bcz tamu undang pas banget woi pas adzan maghrib galau anggap sah puasa\n"
     ]
    }
   ],
   "source": [
    "sample_text = 'Drama kmarin sore : seharian puasa trus pas lg mandi kaget bcz kedatangan tamu tak diundang pas bgt woi pas lagi adzan maghrib trus jd galau dong seharian itu dianggap sah puasa apa enggak ðŸ˜€ https://pic.twitter.com/0sl7DKUKFl'\n",
    "\n",
    "tp = TextPreprocessing(sample_text)\n",
    "print(tp.do_all(sample_text))"
   ]
  },
  {
   "source": [
    "Example data: https://github.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     label                                              tweet  \\\n",
       "0  sadness  Separuh hati ini iri jika melihat seorang anak...   \n",
       "1    happy  Ketika aku tersenyum bukan berarti hidupku sem...   \n",
       "2  sadness  dari mau tdr, tidur, sampe bgn tdr kok yaa kay...   \n",
       "3     love  kan kupeluk engkau erat2 hingga tak ada seoran...   \n",
       "4    anger  Kalo mau ganti presiden itu harus jelas siapa ...   \n",
       "5    happy  Sukses n keren banget dgn no.1 kualitas,bhn kr...   \n",
       "6    anger  Udah mau sarjana 2 kali, mbokya mulut nya jang...   \n",
       "7    anger  Gimana orang ga nilai dr jilbab/syari/nggak ny...   \n",
       "8     fear  Hari ini jadwal presentasi proker di LPPM karn...   \n",
       "9    anger  Foto saya di instagram masih ada cuma lupa pas...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  paruh hati iri lihat orang anak duduk jalan ib...  \n",
       "1    senyum arti hidup sempurna syukur tuhan ikan ku  \n",
       "2  tdr tidur bangun tdr yaa aneh enak hati bawa i...  \n",
       "3  peluk engkau erat orang rebut mu peluk ku sena...  \n",
       "4  ganti presiden calon adu program kerja koalisi...  \n",
       "5  sukses n keren banget nomor kualitasbhn krjait...  \n",
       "6  sarjana kali mbokya mulut nya tinggal ajar sek...  \n",
       "7  orang nilai jilbabsyaringgak nya kadang gemez ...  \n",
       "8  jadwal presentasi proker lppm praktikum ketua ...  \n",
       "9  foto instagram lupa password instanya buka sal...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>clean_tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sadness</td>\n      <td>Separuh hati ini iri jika melihat seorang anak...</td>\n      <td>paruh hati iri lihat orang anak duduk jalan ib...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>happy</td>\n      <td>Ketika aku tersenyum bukan berarti hidupku sem...</td>\n      <td>senyum arti hidup sempurna syukur tuhan ikan ku</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sadness</td>\n      <td>dari mau tdr, tidur, sampe bgn tdr kok yaa kay...</td>\n      <td>tdr tidur bangun tdr yaa aneh enak hati bawa i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>love</td>\n      <td>kan kupeluk engkau erat2 hingga tak ada seoran...</td>\n      <td>peluk engkau erat orang rebut mu peluk ku sena...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>anger</td>\n      <td>Kalo mau ganti presiden itu harus jelas siapa ...</td>\n      <td>ganti presiden calon adu program kerja koalisi...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>happy</td>\n      <td>Sukses n keren banget dgn no.1 kualitas,bhn kr...</td>\n      <td>sukses n keren banget nomor kualitasbhn krjait...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>anger</td>\n      <td>Udah mau sarjana 2 kali, mbokya mulut nya jang...</td>\n      <td>sarjana kali mbokya mulut nya tinggal ajar sek...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>anger</td>\n      <td>Gimana orang ga nilai dr jilbab/syari/nggak ny...</td>\n      <td>orang nilai jilbabsyaringgak nya kadang gemez ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fear</td>\n      <td>Hari ini jadwal presentasi proker di LPPM karn...</td>\n      <td>jadwal presentasi proker lppm praktikum ketua ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>anger</td>\n      <td>Foto saya di instagram masih ada cuma lupa pas...</td>\n      <td>foto instagram lupa password instanya buka sal...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "data = pd.read_csv('twitter_emotion_dataset.csv')\n",
    "tp = TextPreprocessing() \n",
    "data['clean_tweet'] = data['tweet'].apply(tp.do_all)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}